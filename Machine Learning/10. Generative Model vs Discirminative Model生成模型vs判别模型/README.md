## 目录

监督学习方法又分为生成方法(Generative approach)和判别方法(Discriminative approach),所学习到的模型分别称为生成模型(Generative Model)和(Discriminative Model)。
***************
判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k临近(KNN)，感知机(MLP)，决策树(DT)，支持向量机(SVM)，逻辑回归(Logistic Regression)，传统神经网络(Traditional Neural Networks)，条件随机场(Conditional Random Fields)。
主要应用：Image and Document Classification, Biosequence Analysis, Time Series Prediction
判别模型(Discriminative Model)，又称为条件模型，或条件概率模型。估计的是条件概率分布(condition distribution),P(class|context)。利用正负例和分类标签，主要关心判别模型的边缘分布。其目标函数直接对应于分类准确率。（判别模型多数放在分类）
主要特点：寻求不同类别之间的最优分类面，反应的是异类数据之间的差异。
**已知输入变量X,它直接对目标变量Y的条件概率P(Y|X)建模，即计算样本X属于每一类的概率，注意，这里没有假设X服从何种概率分布，而是直接估计出条件概率P(Y|X),典型的代表是Logistic回归和Softmax回归。**
***************
生成模型：由数据学习联合概论密度分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)=P(X,Y)/P(X)。基本思想是首先建立样本的联合概率密度模型P(X,Y),然后得到后验概率P(Y|X)，再利用它进行分类。常见的有朴素贝叶斯(NB)，隐马尔可夫(HMM)模型。
生成模型(Generative Model)，又叫生产式模型。估计的是联合概率分布(Joint Probability Distribution)，P(class, context)=P(class|context)*P(context)。用于随机生成的观察值建模，特别是在给定的某些隐藏参数情况下。在机器学习中，或用于直接对数据建模（用概率密度函数对观察到的样本数据建模），或作为生成条件概率密度函数的中间步骤。通过使用贝叶斯规则可以从生成模型中得到条件分布，如果观察到的数据是完全由生成模型所生成的，那么就可以拟合生成模型的参数，从而尽可能的增加数据相似度。但数据很少能由生成模型完全得到，所以比较准确的方式是直接对条件密度函数建模，即使用分类或回归分析。与描述模型的不同是，描述模型中所有变量都是直接测量得到。
主要特点：一般主要是对后验概率建模，从统计角度表示数据的分布情况，能够反应同类数据本身的相似度。
只关注自己的类本身，不关心到底决策边界在哪。
常见的生成模型有：混合高斯模型Gaussians, Naive Bayes, Mixtures of Multinomials, Mixtures of Gaussians, Mixtures of Experts, 隐马尔科夫模型HMMs, Sigmoidal Belief Networks, Bayesian Networks, Markov Random Fields。
**生成模型最显著的特征是假设样本向量X服从何种概率分布，如正态分布，均匀分布**
***************
所以生成模型和判别模型的主要区别在于：**添加了先验概率**
即：生成模型：P(class, context)=P(class|context)*P(context)
判别模型：P(class|context)

### 优缺点比较

#### 判别模型

- 1、优点：
    - 1.1 与生成模型缺点对应，首先是节省计算资源，另外，需要的样本数量也少于生成模型。
    - 1.2 准确率往往较生成模型高。
    - 1.3 由于直接学习P(Y|X),而不需要求解类别条件概率，所以允许我们对输入进行抽象(比如降维、构造等)，从而能够简化学习。
    - 1.4 分类边界灵活，比使用纯概率方法或生产模型得到的更高级。
    - 1.5 能清晰的分辨出多类或某一类于其他类之间的差异特征。
    - 1.6 在聚类、视角变化、部分遮挡、尺度改变等方面效果较好。
    - 1.7 使用于较多类别的识别。
    - 1.8 判别模型的性能比生成模型简单，比较容易学习。
- 2、缺点：
    - 2.1 不能反应训练数据本身的特性，即能力有限，可以告诉你的是1还是2，但没有办法把整个场景描述出来。
    - 2.2 缺少生成模型的优点，即先验结构不确定性。
    - 2.3 黑盒操作，即变量间的关系不清楚，不可视。

#### 生成模型

- 1、优点：
    - 1.1 生成给出的联合分布P(Y,X),不仅能够由联合分布计算条件分布P(Y|X)(反之则不行),还可以给出其他信息，比如可以使用P(X)=<a href="https://www.codecogs.com/eqnedit.php?latex=\sum_{i=1}^{n}P(Y|X_i)*P(X_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sum_{i=1}^{n}P(Y|X_i)*P(X_i)" title="\sum_{i=1}^{n}P(Y|X_i)*P(X_i)" /></a>来计算边缘分布P(X)。如果一个输入样本的边缘分布P(X)很小的话，那么可以任务学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的outlier detection。
    - 1.2 生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快的收敛于真实模型。
    - 1.3 生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。
    - 1.4 实际上带的信息要比判别模型丰富。
    - 1.5 研究单类问题比判别模型灵活性强。
    - 1.6 模型可以通过增强学习得到。
    - 1.7 能用于数据不完整(Missing Data)情况。
    - 1.8 很容易将先验知识考虑进去。
- 2、缺点：
    - 2.1 天下没有免费的午餐，联合分布能够提供更多的信息，但也需要更多的样本和更多的计算，尤其是为了更准确的估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到的，因而如果我们只需要做分类任务，就浪费了计算资源。
    - 2.2 另外，实践中多数情况下判别模型的效果更好。
    - 2.3 容易会产生错误分类。
    - 2.4 学习和计算过程比较复杂。


#### 综合比较

- 1、一般来讲，生成式模型都会对数据的分布做一定的假设，比如朴素贝叶斯会假设在给定的情况下各个特征直接是相互独立的。当满足这些假设时，生成式模型通常需要很少的数据就能取得不错的效果，但是当这些假设不成立时，判别式模型会得到更好的效果。
- 2、生成式模型最终得到的错误率会比判别式高，但是其需要更少的数据就可以使错误率收敛。
- 3、生成式模型更容易拟合，比如在朴素贝叶斯中只需要记下数就可以，而判别式模型通常都需要解决凸优化问题。
- 4、当添加新的类别时，生成式模型不需要全部重新训练，只需要计算新的类别和的联合分布即可，而判别式模型则需要全部重新训练。
- 5、生成式模型可以更好的利用无标签数据，而判别模型不可以。
- 6、生成模型可以生成，因为判别模型只对数据进行建模，所以判别式模型不可以生成。
- 7、判别式模型可以对输入数据进行预处理，使用处理后的数据来代替，而生成式模型不是很方便进行替换。
- 8、生成模型是已知样本的标签值Y，对样本的特征向量x的条件概率进行建模，即对条件概率P(X|Y)建模，它研究的是每种样本服从何种概率分布。判别模型则刚好相反，已知样本的特征向量X，对样本的标签值Y的概率进行建模，即对条件概率P(Y|X)建模，这种一般用于分量，即给定样本X，计算它属于每个类的概率。
- 根据8的定义，生成模型可以用来根据标签Y生成随机的样本数据X。生成对抗网络(GAN)就是典型的例子，它可以生成服从某种概率分布的随机变量，即拟合类条件概率密度函数P(X|Y)，而此时它的目的不是分类，而是生成样本。事实上，如果我们知道类P(X|Y)或者P(X,Y)，无论用它来做分类还是做数据生成，都是可以的。而判别模型，以及不使用概率模型的判别型分类器则根据样本的特征向量X的值判断它的标签值Y，即用于判断样本的标签值Y。


#### 参考链接

- [知乎-理解生成模型与判别模型](https://zhuanlan.zhihu.com/p/46422895)
- [CSDN-生成模型与判别模型](https://blog.csdn.net/zouxy09/article/details/8195017)